---
title: "Assignment 5"
author: "Emanuele Coradin"
date: "2024-06-03"
output: 
  read_document: rmdformats::readthedown
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 2
  html_document:
    number_sections: true
    theme: spacelab
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

color_vector <- c("#CC0000",   # Dark red
                  "#CC79A7",   # Muted purple
                  "#D55E00",   # Vermilion
                  "#009E73",   # Bluish green
                  "#56B4E9",   # Sky blue
                  '#000046',   # Deep Blue
                  "#DB1E60",   # Pinkish-red
                  "#E69F00")   # Yellow-orange

```

```{r, message=FALSE, echo=FALSE}
library(dplyr)
library(ggplot2)
library(reshape2)
library(latex2exp)
library(coda)
library(rjags)
```

```{r }
#------------- Useful functions -------------
mean_pdf   <- function(f, lower, upper){integrate(function(x) x*f(x), lower, upper,stop.on.error = FALSE)$value}
std_pdf <- function(f, lower, upper) {
  mu <- mean_pdf(f, lower, upper)
  sqrt(integrate(function(x) (x - mu)^2 * f(x), lower, upper, stop.on.error = FALSE)$value / integrate(f, lower, upper, stop.on.error = FALSE)$value)
}
cumulative <- function(f, lower, X){integrate(f, lower, X,stop.on.error = FALSE)$value}
inverse_cumulative <- function(f, p, lower, upper){uniroot(function(x) cumulative(f, lower, x)-p, c(lower, upper))$root}

#inference functions
binom_likelihood <- function(prob, ...) sapply(prob, function(P)  prod(dbinom(prob=P, ...)))
pois_likelihood  <- function(mu, ...)   sapply(mu,   function(MU) prod(dpois (lambda = MU, ...)))
norm_likelihood  <- function(mu, ...)   sapply(mu,   function(MU) prod(dnorm (mean = MU, ...) ))

posterior <- function(parameter, prior, likelihood, lower, upper, ...) {
  unnormalized <- function(x) likelihood(x, ...)*prior(x)
  norm_factor  <- integrate(unnormalized, lower = lower, upper = upper)$value
  unnormalized(parameter)/norm_factor
}

```

# Exercise 1: Markov Chain using Metropolis-Hastings

## Scenario

Given the following un-normalized posterior distribution $g(\theta | x) \propto \frac{1}{2} \exp \left( -\frac{(\theta + 3)^2}{2} \right) + \frac{1}{2} \exp \left( -\frac{(\theta - 3)^2}{2} \right)$:

- Draw a Markov Chain from the posterior distribution using a Metropolis-Hastings algorithm 

- Use a Norm (0, 1) as random-walk candidate density 

- Plot the sampled distribution 

- Analyze the chain with the CODA package and plot the chain autocorrelation 

- Try to use different burn-in cycles and thinning and plot the corresponding posterior distribution and the chain autocorrelation function. What are the best parameters ?
    
## Answers

### Algorithm

1. Initialize the chain at some value $\theta_0$.
2. Draw a random sample $s$ from the distribution $Q(s \mid \theta_t)$. This is often a multivariate Gaussian where $\theta_t$ is the mean, and the covariance matrix specifies the typical size of steps in the chain in each dimension of the parameters $\theta$.
3. Decide whether to accept or not the new candidate sample on the basis of the Metropolis ratio:
   \[
   \rho = \frac{f(s) Q(\theta_t \mid s)}{f(\theta_t) Q(s \mid \theta_t)}
   \]
   - If $\rho \geq 1$, the new candidate is accepted and $\theta_{t+1} = s$.
   - If $\rho < 1$, we only accept it with probability $\rho$:
     - Draw $u \sim U(0, 1)$ and set $\theta_{t+1} = s$ only if $u \leq \rho$.
     - If $s$ is not accepted, we set $\theta_{t+1} = \theta_t$, i.e., the existing sample in the chain is repeated.

```{r metropolis}
#----------------- FUNCTION DEFINITIONS ----------------------
# function to generate the next point in the random walk
std <- 1
Q_gen <- function(theta_t) rnorm(1, theta_t, std)
Q     <- function(x, theta_t) dnorm(x, theta_t, std)

# unnormalized posterior
g <- function(theta) 0.5 * exp(-0.5 * (theta + 3)^2) + 0.5 * exp(-0.5 * (theta - 3)^2)

generate <- 
    function(theta_t){
      s <- Q_gen(theta_t)
      rho <- g(s)*Q(theta_t, s)/(g(theta_t)*Q(s, theta_t))
      ifelse (rho<1, { u <- runif(1); ifelse(u <= rho, s, theta_t)}, s)
    }

Metropolis_Hastings <- function(N, theta_0, burnin = 0, thinning = 1){
  chain <- vector(length = N)
  theta_t <- theta_0
  
  # burn-in phase
  for (ib in 0:burnin) {
      theta_t <- generate(theta_t)
  }
  
  # save phase
  for(step in 1:N){
    #thinning
    for(it in 1:thinning){
      theta_t <- generate(theta_t)
    }
    #append in the chain
    chain[step] <- theta_t
  }
  
  return(chain)
}
```

- Draw a Markov Chain from the posterior distribution using a Metropolis-Hastings algorithm 

- Use a Norm (0, 1) as random-walk candidate density 

- Plot the sampled distribution 

```{r 1.1 - 1.3}
N <- 100000
theta_0 <- 0

chain <- Metropolis_Hastings(N, theta_0)

df <- data.frame(chain = chain)

plt <- ggplot(df, aes(x = chain, y=after_stat(density))) +
  geom_histogram(bins = 50, fill=color_vector[5], alpha = 1., color='black') +
  geom_density(color = color_vector[7], size = 1) +
  labs(title = "Histogram and Density of Metropolis-Hastings Chain", x = expression(theta), y = "Posterior")

# Display the plot
print(plt)
```

- Analyze the chain with the CODA package and plot the chain autocorrelation 

- Try to use different burn-in cycles and thinning and plot the corresponding posterior distribution and the chain autocorrelation function. What are the best parameters ?

```{r 1.4}
# Convert the chain to an mcmc object
mcmc_chain <- mcmc(chain)

lags <- seq(0, 500, 10)
autocorr_chain <- autocorr(mcmc_chain, lags = lags)

plot(lags, autocorr_chain, type = 's', col=color_vector[7], lty=1, lwd=2, main = 'Autocorrelation of the chain', xlab = 'lags', ylab='autocorrelation')
```
```

#{r 1.5}

burnin_list   <- list(100, 1000, 10000)
thinning_list <- list(5, 10, 50)

chain_list <- unlist(lapply(burnin_list, function(burnin) lapply(thinning_list, function(thinning) Metropolis_Hastings(N, theta_0, burnin, thinning))), recursive=FALSE)

# Save the chain_list to a file
SaveRDS(chain_list, 'chain_list_RDS.RData')

```

```{r 1.5 plots}
burnin_list   <- list(100, 1000, 10000)
thinning_list <- list(5, 10, 50)

# Retrieve the precomputed chains
chain_list_RDS <- readRDS("chain_list_RDS.RData")

# Plot the autocorrelation

mcmc_chain_list <- lapply(chain_list_RDS,  mcmc)
N_chains <- length(mcmc_chain_list)
  
lags <- seq(0, 150, 5)
autocorr_chain_list <- lapply(mcmc_chain_list, function(mcmc_chain) log(abs(autocorr(mcmc_chain, lags = lags))))

ylim=c(min(unlist(autocorr_chain_list)), max(unlist(autocorr_chain_list))*1.1)

vanilla_mcmc <- log(abs(autocorr(mcmc_chain, lags = lags)))

colormap <- rainbow(N_chains)

labels <- unlist(lapply(burnin_list, function(burnin) 
                 lapply(thinning_list, function(thinning) 
                 paste("B =", burnin, ", T =", thinning))))

plot(lags, vanilla_mcmc, type = 's', col=color_vector[7], lty=1, lwd=2, main = 'Autocorrelation of the chains', xlab = 'lags', ylab='log(|autocorrelation|)', xlim = c(0,205), ylim = ylim)

void <- sapply(1:N_chains, function(iline) lines(lags, autocorr_chain_list[[iline]], type = 's', col=colormap[iline], lty=iline, lwd=2))

# Place the legend outside the plot area, using multiple columns
legend('topright', legend = labels, col = colormap, lty = 1:N_chains, lwd = 2, ncol = 1, cex = 0.8)

# Plot the posterior
plot_posterior <- function(chain, color){
  hist(chain, breaks = c(-Inf, posterior_breaks, Inf), freq = FALSE, col = NULL, border = color, add=TRUE, xlab = expression(theta), ylab = 'density', main = 'Histogram of the posteriors from different chains')
}

posterior_breaks <- hist(x=chain, breaks = 50, freq = FALSE, col = NULL, border = color_vector[7], xlim = c(-7, 10))$breaks

void <- mapply(plot_posterior, chain_list_RDS, colormap)
legend('topright', legend = labels, col = colormap, lty = 1, lwd = 2, ncol = 1, cex = 0.8)

```

From this plot we can see that it seems that every chain reaches convergence, no matter the different level of autocorrelation.

# Exercise 2: MCMC inference on Linear model

## Scenario

A set of measured data should follow, according to the physics model applied to them, a linear behavior. 
Data are the following: Y = { -7.821 -1.494 -15.444 -10.807 -13.735 -14.442 -15.892 -18.326 } X = { 5 6 7 8 9 10 11 12 }

Tasks:

- Perform a simple linear regression model running a Markov Chain Monte Carlo with JAGS, assuming that data follow the model: Z[i] = a + b * X[i]; and the likelihood of the measured data follow a Gaussian likelihood distribution: Y[i] dnorm(Z[i], c) (you can constrain the parameter a, b and c to the following intervals: a ∈ [1, 10], b ∈ [−1, 3] and c ∈ [0.034, 4]) 

- Run JAGS experimenting with the burn-in and number of iterations of the chain. Plot the evolution of the chains and the posterior distributions of a and b. Compute the 95% credibility interval for the parameters. 

- Using the obtained posterior distributions, compute the posterior distribution of $\sigma = \sqrt{(\frac{1}{c})}$.
  
## Answers

```{r useful plot function}
plot_intervals <- function(histogram){
  # Compute:
  xlim  = c(min(histogram$breaks), max(histogram$breaks))
  step_posterior <- stepfun(histogram$breaks, c(0, histogram$density, 0))
  mean_posterior <- mean_pdf(step_posterior, lower = xlim[1], upper = xlim[2])
  std_posterior  <- std_pdf (step_posterior, lower = xlim[1], upper = xlim[2])
  interval_95    <- sapply(c(0.025, 0.975), function(P) inverse_cumulative(step_posterior, p = P, lower = xlim[1], upper = xlim[2]))
  
  # Plot:
  x_plot <- seq(from=interval_95[1], to=interval_95[2], length.out=500)
  y_plot <- c(0, step_posterior(x_plot), 0)
  x_plot <- c(interval_95[1], x_plot, interval_95[2])
  polygon(x_plot, y_plot, col = adjustcolor(color_vector[7], alpha.f = 0.25),border = NA)
  
  abline(v=mean_posterior, col = color_vector[1], lwd=2, lty='longdash')
  abline(v=mean_posterior - std_posterior, col = color_vector[6], lwd=2, lty='dashed')
  abline(v=mean_posterior + std_posterior, col = color_vector[6], lwd=2, lty='dashed')
  
  legend("topright", legend = c("Mean", "±std"), col = c(color_vector[1], color_vector[6]), lty = c('longdash', 'dashed'))
  
  legend("right", legend="95%% credibility interval", fill=color_vector[7])
  
  return()
  
}
```

```{r 2}
# Defining data
X <- c(5, 6, 7, 8, 9, 10, 11, 12)
Y <- c(-7.821, -1.494, -15.444, -10.807, -13.735, -14.442, -15.892, -18.326) 
N_burnin <- 2000 # length of the burn-in phase 
thinning <- 7
Nrep = 100000     # number of values to simulate

# Let' define the model
model_string <- "model{
  
  # Model: Z[i] = a + b * X[i];
  # Likelihood: Y[i] ~dnorm(Z[i], c)
  # a ∈ [1, 10], b ∈ [−1, 3] and c ∈ [0.034, 4]) 

  # Likelihood
  for (i in 1:length(X)) {
    Z[i] <- a + b * X[i]
    Y[i] ~ dnorm(Z[i], c)
  }

  # Prior
  a ~ dunif(-3, 10)
  b ~ dunif(-3, 3)
  c ~ dunif(0.034, 4)
  
}"

# Compile jags model
dataList = list(X = X, Y = Y)
model <- jags.model(file = textConnection(model_string), 
                    data = dataList)

# Add burnin
update(model, n.iter = N_burnin)

# Sample the posterior
posterior_sample <- coda.samples(model,
                       variable.names = c("a", "b", "c"),
                       n.iter = Nrep, thin = thinning)
summary(posterior_sample)
plot(posterior_sample)

posterior_matrix <- as.matrix(posterior_sample)
# Retrieve the chains
a_samples <- posterior_matrix[, "a"]
b_samples <- posterior_matrix[, "b"]
c_samples <- posterior_matrix[, "c"]

# Set up the plotting area to have 2 rows and 2 column
par(mfrow = c(2, 2))  

acf(a_samples, main = "Autocorrelation of a")
acf(b_samples, main = "Autocorrelation of b")
acf(c_samples, main = "Autocorrelation of c")
```

```{r 2 hist}
# Computing the histograms and the step functions

a_hist <- hist(a_samples, breaks = 30, main = "Posterior Distribution of a", xlab = "a", freq = FALSE, col = NULL, border = color_vector[6])
a_best <- a_hist$mids[ which.max(a_hist$density) ]
plot_intervals(a_hist)

b_hist <- hist(b_samples, breaks = 30, main = "Posterior Distribution of b", xlab = "b", freq = FALSE, col = NULL, border = color_vector[6], xlim=c(-3, 0))
b_best <- b_hist$mids[ which.max(b_hist$density) ]
plot_intervals(b_hist)

c_hist <- hist(c_samples, breaks = 30, main = "Posterior Distribution of c", xlab = "c", freq = FALSE, col = NULL, border = color_vector[6])
c_best <- c_hist$mids[ which.max(c_hist$density) ]
plot_intervals(c_hist)

# ----- Linear Fit plot -----

plot(X, Y, main="Linear fit with Bayesian inference", col=color_vector[6], pch=19)
curve(b_best*x+a_best, from = 0, to = 13, add=TRUE, col=color_vector[1], lwd=2)


```

```{r 2.3}
sigma_samples <- sqrt(1/c_samples)

hist(sigma_samples, breaks = 50, freq=FALSE, main="Posterior Distribution of sigma", xlab=expression(sigma), col=color_vector[5])
```


# Exercise 3 MCMC inference on Gaussian model

## Scenario

Suppose we observe the following values x = 2.06, 5.56, 7.93, 6.56, 2.05 and we assume that the data come from a Gaussian distribution with unknown mean m and variance s2 

- Build a simple JAGS model and run a Markov Chain Monte Carlo to obtain the posterior distribution of the mean and variance.

- Assume uniform prior distributions for the parameters, m dunif(-10, 10) and s dunif(0,50).

- Compute also the posterior distribution for m/s.

## Answers

```{r 3}
# Defining data
X <- c( 2.06, 5.56, 7.93, 6.56, 2.05)

N_burnin <- 1000 # length of the burn-in phase 
thinning <- 1
Nrep = 100000     # number of values to simulate

# Let' define the model
model_string <- "model{
  
  ratio <- mu/s2
  
  # Likelihood
  for (i in 1:length(X)) {
    X[i] ~ dnorm(mu, s2)
  }

  # Prior
  mu ~ dunif(-10, 10)
  s2 ~ dunif(0, 50)
  
}"

# Compile jags model
dataList = list(X = X)
model <- jags.model(file = textConnection(model_string), 
                    data = dataList)

# Add burnin
update(model, n.iter = N_burnin)

# Sample the posterior
posterior_sample <- coda.samples(model,
                       variable.names = c("mu", "s2", "ratio"),
                       n.iter = Nrep, thin = thinning)
summary(posterior_sample)
plot(posterior_sample)

# Retrieve the samples
posterior_matrix <- as.matrix(posterior_sample)

mu_samples <- posterior_matrix[, "mu"]
s2_samples <- posterior_matrix[, "s2"]
ratio_samples <- posterior_matrix[, "ratio"]

# Set up the plotting area to have 2 rows and 2 column
par(mfrow = c(2, 2))  

acf(mu_samples, main = "Autocorrelation of mu")
acf(s2_samples, main = "Autocorrelation of s2")
acf(ratio_samples, main = "Autocorrelation of ratio")
```

```{r 3 plot}
# Computing the histograms and the step functions

mu_hist <- hist(mu_samples, breaks = 50, main = "Posterior Distribution of mu", xlab = expression(mu), freq = FALSE, col = NULL, border = color_vector[6], xlim=c(-2, 12))
plot_intervals(mu_hist)

s2_hist <- hist(s2_samples, breaks = 50, main = "Posterior Distribution of s2", xlab = "s2", freq = FALSE, col = NULL, border = color_vector[6])
plot_intervals(s2_hist)

ratio_hist <- hist(ratio_samples, breaks = 300, main = "Posterior Distribution of the ratio", xlab = "ratio", freq = FALSE, col = NULL, border = color_vector[6], xlim = c(0,200))
plot_intervals(ratio_hist)


# Autocorrelation plot
lags <- seq(0, 150, 5)
autocorrelation <- autocorr(mcmc_chain, lags = lags)

plot(lags, autocorrelation, type = 's', col=color_vector[7], lty=1, lwd=2, main = 'Autocorrelation of the chain', xlab = 'lags', ylab= 'autocorrelation') #'log(|autocorrelation|)'

```

# Exercise 4 MCMC and Edwin Hubble law

## Scenario

The data set that Edwin Hubble used to show that galaxies are moving either away or towards us are given in the following table:
  
  | D [parsec] | V [km/s]|
  |------------|---------|
  |   0.0032   |   170   |
  |   0.0034   |   290   |
  |   0.2140   |   -130  |
  |   0.2630   |   -70   |
  |   0.2750   |   -185  |
  |------------|---------|
  |   0.2750   |   -220  |
  |   0.4500   |   200   |
  |   0.5000   |   290   |
  |   0.5000   |   270   |
  |   0.6300   |   200   |
  |------------|---------|
  |   0.8000   |   920   |
  |   0.9000   |   450   |
  |   0.9000   |   500   |
  |   0.9000   |   500   |
  |   0.9000   |   960   |
  |------------|---------|
  |   2.0000   |   500   |
  |   2.0000   |   850   |
  |   2.0000   |   800   |
  |   2.0000   |   1090  |
    

Using this data set define a JAGS model to fit data with the following:
    
- V[i] from dnorm(b * D[i], c), where V represent the velocity in units of km/s, D is the observed distance (in units of parsec), and b and c are two parameters of the model 

- Assume whatever prior distribution you think is appropriate plot the evolution of the chains, the posterior distribution of the parameters and the 95% credibility interval


## Answers

```{r 4}
# Defining data
D <- c( 0.0032,	0.0034,	0.214,	0.263,	0.275,	0.275,	0.45,	0.5,	0.5,	0.63,	0.8,	0.9,	0.9,	0.9,	0.9,	2,	2,	2,	2)

V <- c(170,	290,	-130,	-70,	-185,	-220,	200,	290,	270,	200,	920,	450,	500,	500,	960,	500,	850,	800,	1090)

N_burnin <- 1000 # length of the burn-in phase 
thinning <- 1
Nrep = 100000     # number of values to simulate

# Let' define the model
model_string <- "model{
  
  # Likelihood
  for (i in 1:length(D)) {
    V[i] ~ dnorm(b*D[i], c)
  }

  # Prior
  b ~ dunif(0, 1200)
  c ~ dunif(0, 0.001)
  
}"

# Compile jags model
dataList = list(D = D, V=V)
model <- jags.model(file = textConnection(model_string), 
                    data = dataList)

# Add burnin
update(model, n.iter = N_burnin)

# Sample the posterior
posterior_sample <- coda.samples(model,
                       variable.names = c("b", "c"),
                       n.iter = Nrep, thin = thinning)
summary(posterior_sample)
plot(posterior_sample)

# Retrieve the samples
posterior_matrix <- as.matrix(posterior_sample)

b_samples <- posterior_matrix[, "b"]
c_samples <- posterior_matrix[, "c"]

# Set up the plotting area to have 1 rows and 2 column
par(mfrow = c(1, 2))  

acf(b_samples, main = "Autocorrelation of b")
acf(c_samples, main = "Autocorrelation of c")

```
```{r 4 plot}

b_hist <- hist(b_samples, breaks = 30, main = "Posterior Distribution of b", xlab = "b", freq = FALSE, col = NULL, border = color_vector[6], xlim = c(200,800))
plot_intervals(b_hist)
b_best <- b_hist$mids[ which.max(b_hist$density) ]


c_hist <- hist(c_samples, breaks = 30, main = "Posterior Distribution of c", xlab = "c", freq = FALSE,  col = NULL, border = color_vector[6])
plot_intervals(c_hist)
c_best <- c_hist$mids[ which.max(c_hist$density) ]


plot(D, V, main="Fit Hubble Law", col=color_vector[6], pch=19)
curve(b_best*x, from = 0, to = 13, add=TRUE, col=color_vector[7], lwd=2)

```